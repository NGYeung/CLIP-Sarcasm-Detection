{
   "cells":[
      {
         "cell_type":"code",
         "execution_count":1,
         "id":"7ea709ec",
         "metadata":{
            
         },
         "outputs":[
            
         ],
         "source":[
            "import torch\n",
            "torch.cuda.empty_cache()"
         ]
      },
      {
         "cell_type":"code",
         "execution_count":2,
         "id":"8fca9d5b",
         "metadata":{
            
         },
         "outputs":[
            
         ],
         "source":[
            "import warnings\n",
            "warnings.filterwarnings('ignore')\n",
            "from custom_models.clip_class import I_T_ContrastiveLoss, clip_for_meme\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type":"code",
         "execution_count":3,
         "id":"84cd2eb6",
         "metadata":{
            
         },
         "outputs":[
            
         ],
         "source":[
            "import CLIP_classifier2 as clip_c"
         ]
      },
      {
         "cell_type":"code",
         "execution_count":null,
         "id":"ef073a7f",
         "metadata":{
            "scrolled":true
         },
         "outputs":[
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "\n",
                  " \t ----------- Model = BertDistilled-DINOv2-CLIP ------------\n"
               ]
            },
            {
               "name":"stderr",
               "output_type":"stream",
               "text":[
                  "Using cache found in /home/yiyangl/.cache/torch/hub/facebookresearch_dinov2_main\n"
               ]
            },
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "\n",
                  " \t Running on CPU.\n",
                  "\n",
                  " \t ----------- Model Loaded------------\n",
                  "\n",
                  " \t ------------ loading epoch  4  ------------\n",
                  "\n",
                  " \t ----------- Model Loaded------------\n",
                  "\t *Total Params* =  162580481\n",
                  "\t *Trainable Params* =  9637121\n",
                  "gradient stats:\n",
                  "module.img_projection.1.weight - Mean: 0.000767868768889457, Max: 0.01254372950643301, Min:-0.011224641464650631 ,Grad Norm: 0.6873851418495178\n",
                  "module.img_projection.1.bias - Mean: 0.00035443317028693855, Max: 0.0025411322712898254, Min:-0.0016033585416153073 ,Grad Norm: 0.01092879381030798\n",
                  "module.img_projection.3.weight - Mean: 0.0009746466530486941, Max: 0.008839272893965244, Min:-0.010775012895464897 ,Grad Norm: 0.49329331517219543\n",
                  "module.img_projection.3.bias - Mean: 0.00033006799640133977, Max: 0.001222803257405758, Min:-0.0009762839181348681 ,Grad Norm: 0.006657047662883997\n",
                  "module.img_projection.4.weight - Mean: 0.0014449104201048613, Max: 0.008023791015148163, Min:-0.0064774202182888985 ,Grad Norm: 0.030385412275791168\n",
                  "module.img_projection.4.bias - Mean: 0.000845228205434978, Max: 0.003102018963545561, Min:-0.002379096345975995 ,Grad Norm: 0.01712965779006481\n",
                  "module.txt_projection.1.weight - Mean: 0.0007910581771284342, Max: 0.03448173403739929, Min:-0.03973575681447983 ,Grad Norm: 0.8307611346244812\n",
                  "module.txt_projection.1.bias - Mean: 0.0012916293926537037, Max: 0.008387290872633457, Min:-0.009184478782117367 ,Grad Norm: 0.04136946052312851\n",
                  "module.txt_projection.3.weight - Mean: 0.000961383106186986, Max: 0.01679777167737484, Min:-0.016844231635332108 ,Grad Norm: 0.5815038681030273\n",
                  "module.txt_projection.3.bias - Mean: 0.0019768360070884228, Max: 0.007009580731391907, Min:-0.0065841153264045715 ,Grad Norm: 0.03975139930844307\n",
                  "module.txt_projection.4.weight - Mean: 0.0015568770468235016, Max: 0.008193272165954113, Min:-0.010485717095434666 ,Grad Norm: 0.03442729264497757\n",
                  "module.txt_projection.4.bias - Mean: 0.0015152744017541409, Max: 0.005364987067878246, Min:-0.005034722853451967 ,Grad Norm: 0.030407607555389404\n",
                  "module.proj_into_class.0.weight - Mean: 0.0010346408234909177, Max: 0.03013351559638977, Min:-0.030589528381824493 ,Grad Norm: 5.00389289855957\n",
                  "module.proj_into_class.0.bias - Mean: 3.513032424962148e-07, Max: 1.9744038581848145e-06, Min:-2.1457672119140625e-06 ,Grad Norm: 6.2629683270643e-06\n",
                  "module.proj_into_class.1.weight - Mean: 0.0018232977017760277, Max: 0.005347657483071089, Min:-0.00868446659296751 ,Grad Norm: 0.030411982908844948\n",
                  "module.proj_into_class.1.bias - Mean: 0.01666705310344696, Max: 0.04674345627427101, Min:-0.05314968153834343 ,Grad Norm: 0.23359085619449615\n",
                  "module.proj_into_class.2.weight - Mean: 0.017641184851527214, Max: 0.0432831309735775, Min:-0.06752929836511612 ,Grad Norm: 0.2543095052242279\n",
                  "module.proj_into_class.2.bias - Mean: 0.15482325851917267, Max: 0.15482325851917267, Min:0.15482325851917267 ,Grad Norm: 0.15482325851917267\n",
                  "train_loss =  0.24561317265033722\n"
               ]
            },
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "gradient stats:\n",
                  "module.img_projection.1.weight - Mean: 0.0005975133390165865, Max: 0.009525016881525517, Min:-0.010381697677075863 ,Grad Norm: 0.5314698219299316\n",
                  "module.img_projection.1.bias - Mean: 0.00026481651002541184, Max: 0.0010129575384780765, Min:-0.0018849263433367014 ,Grad Norm: 0.00797562301158905\n",
                  "module.img_projection.3.weight - Mean: 0.0007584356935694814, Max: 0.008114585652947426, Min:-0.008603216148912907 ,Grad Norm: 0.3799997568130493\n",
                  "module.img_projection.3.bias - Mean: 0.00027592977858148515, Max: 0.0008337608887813985, Min:-0.0008509257459081709 ,Grad Norm: 0.005457066930830479\n",
                  "module.img_projection.4.weight - Mean: 0.0010868774261325598, Max: 0.005179098807275295, Min:-0.005304097197949886 ,Grad Norm: 0.02364945225417614\n",
                  "module.img_projection.4.bias - Mean: 0.0006742289406247437, Max: 0.0022695434745401144, Min:-0.001993961865082383 ,Grad Norm: 0.013419589027762413\n",
                  "module.txt_projection.1.weight - Mean: 0.0005757540930062532, Max: 0.014664746820926666, Min:-0.01953022927045822 ,Grad Norm: 0.5959797501564026\n",
                  "module.txt_projection.1.bias - Mean: 0.000835991115309298, Max: 0.0037154662422835827, Min:-0.004004602320492268 ,Grad Norm: 0.02547473832964897\n",
                  "module.txt_projection.3.weight - Mean: 0.0006814707885496318, Max: 0.009235710836946964, Min:-0.010087791830301285 ,Grad Norm: 0.3998423218727112\n",
                  "module.txt_projection.3.bias - Mean: 0.0011843673419207335, Max: 0.003289084415882826, Min:-0.003310767002403736 ,Grad Norm: 0.022854682058095932\n",
                  "module.txt_projection.4.weight - Mean: 0.0011735913576558232, Max: 0.005310375243425369, Min:-0.0053651961497962475 ,Grad Norm: 0.02497253194451332\n",
                  "module.txt_projection.4.bias - Mean: 0.0009195085731334984, Max: 0.0025729089975357056, Min:-0.0029763001948595047 ,Grad Norm: 0.017887648195028305\n",
                  "module.proj_into_class.0.weight - Mean: 0.0007522739470005035, Max: 0.022756995633244514, Min:-0.022642487660050392 ,Grad Norm: 3.624884843826294\n",
                  "module.proj_into_class.0.bias - Mean: 3.85692146664951e-07, Max: 2.1383166313171387e-06, Min:-1.8030405044555664e-06 ,Grad Norm: 6.5257590904366225e-06\n",
                  "module.proj_into_class.1.weight - Mean: 0.0016528021078556776, Max: 0.003417555708438158, Min:-0.011184955015778542 ,Grad Norm: 0.029389025643467903\n",
                  "module.proj_into_class.1.bias - Mean: 0.017847925424575806, Max: 0.05005084350705147, Min:-0.056911651045084 ,Grad Norm: 0.2501320540904999\n",
                  "module.proj_into_class.2.weight - Mean: 0.015697922557592392, Max: 0.05202537775039673, Min:-0.04204987362027168 ,Grad Norm: 0.214829683303833\n",
                  "module.proj_into_class.2.bias - Mean: 0.16577398777008057, Max: 0.16577398777008057, Min:0.16577398777008057 ,Grad Norm: 0.16577398777008057\n",
                  "train_loss =  0.6831084489822388\n",
                  "\n",
                  "\t Epoch :  6\n",
                  "\n",
                  "\t Accuracy :  0.6978\n",
                  "\n",
                  "\t AUROC :  0.8179981366410167\n",
                  "\t Training loss:  0.6175\n",
                  "\t Training time current epoch:  6107.61 seconds\n"
               ]
            }
         ],
         "source":[
            "#testrun!\n",
            "\n",
            "clip_c.train(batch_size = 96, epochs=6,load = 4, ratio = 1, loss = 1)"
         ]
      },
      {
         "cell_type":"code",
         "execution_count":5,
         "id":"24d98555",
         "metadata":{
            
         },
         "outputs":[
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "\n",
                  " \t ----------- Model = BertDistilled-DINOv2-CLIP ------------\n"
               ]
            },
            {
               "name":"stderr",
               "output_type":"stream",
               "text":[
                  "Using cache found in /home/yiyangl/.cache/torch/hub/facebookresearch_dinov2_main\n"
               ]
            },
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "\n",
                  " \t Running on CPU.\n",
                  "\n",
                  " \t ----------- Model Loaded------------\n",
                  "\n",
                  " \t ------------ loading epoch  6  ------------\n",
                  "\n",
                  " \t ----------- Model Loaded------------\n",
                  "\t *Total Params* =  162580481\n",
                  "\t *Trainable Params* =  9637121\n",
                  "gradient stats:\n",
                  "module.img_projection.1.weight - Mean: 0.0005585695616900921, Max: 0.010674445889890194, Min:-0.011126306839287281 ,Grad Norm: 0.5157875418663025\n",
                  "module.img_projection.1.bias - Mean: 0.00026772584533318877, Max: 0.001123400405049324, Min:-0.001104455441236496 ,Grad Norm: 0.008132252842187881\n",
                  "module.img_projection.3.weight - Mean: 0.0006960504688322544, Max: 0.011096963658928871, Min:-0.011434911750257015 ,Grad Norm: 0.37314367294311523\n",
                  "module.img_projection.3.bias - Mean: 0.00024808302987366915, Max: 0.0008638284634798765, Min:-0.0008285375079140067 ,Grad Norm: 0.005002553574740887\n",
                  "module.img_projection.4.weight - Mean: 0.0010343885514885187, Max: 0.005369951948523521, Min:-0.0042133694514632225 ,Grad Norm: 0.022880785167217255\n",
                  "module.img_projection.4.bias - Mean: 0.0005963142029941082, Max: 0.0019035979639738798, Min:-0.0022286612074822187 ,Grad Norm: 0.011881484650075436\n",
                  "module.txt_projection.1.weight - Mean: 0.0005526370950974524, Max: 0.028471458703279495, Min:-0.03478914499282837 ,Grad Norm: 0.6055046916007996\n",
                  "module.txt_projection.1.bias - Mean: 0.0008969567134045064, Max: 0.0053459410555660725, Min:-0.00617110775783658 ,Grad Norm: 0.03022030182182789\n",
                  "module.txt_projection.3.weight - Mean: 0.0006825219024904072, Max: 0.014294774271547794, Min:-0.01207705307751894 ,Grad Norm: 0.4103858470916748\n",
                  "module.txt_projection.3.bias - Mean: 0.0013515539467334747, Max: 0.00524724368005991, Min:-0.005557965487241745 ,Grad Norm: 0.02659526653587818\n",
                  "module.txt_projection.4.weight - Mean: 0.0011173200327903032, Max: 0.00417027622461319, Min:-0.004905248060822487 ,Grad Norm: 0.023222599178552628\n",
                  "module.txt_projection.4.bias - Mean: 0.0010360918240621686, Max: 0.004135298542678356, Min:-0.004242586437612772 ,Grad Norm: 0.020475132390856743\n",
                  "module.proj_into_class.0.weight - Mean: 0.0006679429789073765, Max: 0.030891740694642067, Min:-0.028687609359622 ,Grad Norm: 3.2904839515686035\n",
                  "module.proj_into_class.0.bias - Mean: 1.7576212485437281e-07, Max: 1.1920928955078125e-06, Min:-7.301568984985352e-07 ,Grad Norm: 2.9645245831488864e-06\n",
                  "module.proj_into_class.1.weight - Mean: 0.001464174478314817, Max: 0.0026718603912740946, Min:-0.009044894017279148 ,Grad Norm: 0.025834910571575165\n",
                  "module.proj_into_class.1.bias - Mean: 0.010595305822789669, Max: 0.02970314957201481, Min:-0.033770568668842316 ,Grad Norm: 0.14845964312553406\n",
                  "module.proj_into_class.2.weight - Mean: 0.012736664153635502, Max: 0.03489096462726593, Min:-0.044392798095941544 ,Grad Norm: 0.1803252249956131\n",
                  "module.proj_into_class.2.bias - Mean: 0.09835503250360489, Max: 0.09835503250360489, Min:0.09835503250360489 ,Grad Norm: 0.09835503250360489\n"
               ]
            },
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "gradient stats:\n",
                  "module.img_projection.1.weight - Mean: 0.0004845878866035491, Max: 0.010130146518349648, Min:-0.010196160525083542 ,Grad Norm: 0.4478608965873718\n",
                  "module.img_projection.1.bias - Mean: 0.00021558627486228943, Max: 0.0012451477814465761, Min:-0.0015820908593013883 ,Grad Norm: 0.006757316179573536\n",
                  "module.img_projection.3.weight - Mean: 0.0006157410680316389, Max: 0.00855721440166235, Min:-0.007743388880044222 ,Grad Norm: 0.3199087381362915\n",
                  "module.img_projection.3.bias - Mean: 0.00019599679217208177, Max: 0.0005987716722302139, Min:-0.0007615965441800654 ,Grad Norm: 0.00394437275826931\n",
                  "module.img_projection.4.weight - Mean: 0.0008507836610078812, Max: 0.005328497849404812, Min:-0.00400906428694725 ,Grad Norm: 0.01896362565457821\n",
                  "module.img_projection.4.bias - Mean: 0.000503944291267544, Max: 0.0014752147253602743, Min:-0.00195997953414917 ,Grad Norm: 0.010201656259596348\n",
                  "module.txt_projection.1.weight - Mean: 0.00044739976874552667, Max: 0.015113512985408306, Min:-0.013855262659490108 ,Grad Norm: 0.47093454003334045\n",
                  "module.txt_projection.1.bias - Mean: 0.0006752551416866481, Max: 0.00392175605520606, Min:-0.0028771255165338516 ,Grad Norm: 0.021502582356333733\n",
                  "module.txt_projection.3.weight - Mean: 0.0005517107783816755, Max: 0.008145151659846306, Min:-0.009558506309986115 ,Grad Norm: 0.32961076498031616\n",
                  "module.txt_projection.3.bias - Mean: 0.0010600518435239792, Max: 0.003521369770169258, Min:-0.0036435751244425774 ,Grad Norm: 0.021296493709087372\n",
                  "module.txt_projection.4.weight - Mean: 0.0009516789577901363, Max: 0.004154059570282698, Min:-0.005504232831299305 ,Grad Norm: 0.021160485222935677\n",
                  "module.txt_projection.4.bias - Mean: 0.0007733565289527178, Max: 0.0025936244055628777, Min:-0.002646377310156822 ,Grad Norm: 0.015540538355708122\n",
                  "module.proj_into_class.0.weight - Mean: 0.0005514839431270957, Max: 0.016587402671575546, Min:-0.015227533876895905 ,Grad Norm: 2.672740936279297\n",
                  "module.proj_into_class.0.bias - Mean: 2.9404873203020543e-07, Max: 1.1771917343139648e-06, Min:-1.296401023864746e-06 ,Grad Norm: 4.71990961159463e-06\n",
                  "module.proj_into_class.1.weight - Mean: 0.001425701193511486, Max: 0.002929999493062496, Min:-0.010292157530784607 ,Grad Norm: 0.023824471980333328\n",
                  "module.proj_into_class.1.bias - Mean: 0.012048253789544106, Max: 0.0337698757648468, Min:-0.038393642753362656 ,Grad Norm: 0.1687997281551361\n",
                  "module.proj_into_class.2.weight - Mean: 0.012872934341430664, Max: 0.02912800945341587, Min:-0.040731385350227356 ,Grad Norm: 0.1699160784482956\n",
                  "module.proj_into_class.2.bias - Mean: 0.11180512607097626, Max: 0.11180512607097626, Min:0.11180512607097626 ,Grad Norm: 0.11180512607097626\n",
                  "train_loss =  0.7091594934463501\n",
                  "\n",
                  "\t Epoch :  8\n",
                  "\n",
                  "\t Accuracy :  0.7037\n",
                  "\n",
                  "\t AUROC :  0.8202080219993153\n",
                  "\n",
                  "\t f1 :  0.6801835918265646\n",
                  "\t Training loss:  0.6398\n",
                  "\t Training time current epoch:  6323.42 seconds\n"
               ]
            },
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  
               ]
            },
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "train_loss =  0.6744143104553223\n",
                  "\n",
                  "\t Epoch :  9\n",
                  "\n",
                  "\t Accuracy :  0.7379\n",
                  "\n",
                  "\t AUROC :  0.8197169658569992\n",
                  "\n",
                  "\t f1 :  0.7171293248302494\n",
                  "\t Training loss:  0.6378\n",
                  "\t Training time current epoch:  6361.75 seconds\n"
               ]
            },
            {
               "ename":"KeyboardInterrupt",
               "evalue":"",
               "output_type":"error",
               "traceback":[
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclip_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mload\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/PY_PROJECTS/MemeProjects/CLIP_classifier2.py:301\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch_size, epochs, ratio, load, train_loader, loss)\u001b[0m\n\u001b[1;32m    297\u001b[0m     epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    300\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_for_meme.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m epochloss, auroc, acc, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, epochloss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m accuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m, acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m'\u001b[39m, auroc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, f1)\n",
                  "File \u001b[0;32m~/PY_PROJECTS/MemeProjects/CLIP_classifier2.py:177\u001b[0m, in \u001b[0;36mclassification\u001b[0;34m(test_loader, model, loss_fn, device)\u001b[0m\n\u001b[1;32m    173\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m#Forward\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m class_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m classlist\u001b[38;5;241m.\u001b[39mappend(class_\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m    180\u001b[0m labellist\u001b[38;5;241m.\u001b[39mappend(labels\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat())\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                  "File \u001b[0;32m~/PY_PROJECTS/MemeProjects/custom_models/clip_class.py:269\u001b[0m, in \u001b[0;36mclip_for_meme.forward\u001b[0;34m(self, images, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, input_ids, attention_mask):\n\u001b[0;32m--> 269\u001b[0m     image_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    sns.heatmap(image_embed)\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    plt.show('image_embedding')\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     emb_len \u001b[38;5;241m=\u001b[39m image_embed\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                  "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325\u001b[0m, in \u001b[0;36mDinoVisionTransformer.forward\u001b[0;34m(self, is_training, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 325\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
                  "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:258\u001b[0m, in \u001b[0;36mDinoVisionTransformer.forward_features\u001b[0;34m(self, x, masks)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_features_list(x, masks)\n\u001b[0;32m--> 258\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_tokens_with_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    261\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x)\n",
                  "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:215\u001b[0m, in \u001b[0;36mDinoVisionTransformer.prepare_tokens_with_masks\u001b[0;34m(self, x, masks)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_tokens_with_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    214\u001b[0m     B, nc, w, h \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 215\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(masks\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), x)\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                  "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:75\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m H \u001b[38;5;241m%\u001b[39m patch_H \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image height \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a multiple of patch height \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_H\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m W \u001b[38;5;241m%\u001b[39m patch_W \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image width \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a multiple of patch width: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_W\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# B C H W\u001b[39;00m\n\u001b[1;32m     76\u001b[0m H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# B HW C\u001b[39;00m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/.conda/envs/conda-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
               ]
            }
         ],
         "source":[
            "# RUN!!!!! \n",
            "clip_c.train(batch_size = 96, epochs=9,load = 6, ratio = 1, loss = 1)"
         ]
      },
      {
         "cell_type":"code",
         "execution_count":4,
         "id":"20781ff0",
         "metadata":{
            
         },
         "outputs":[
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "\n",
                  " \t ----------- Model = BertDistilled-DINOv2-CLIP ------------\n"
               ]
            },
            {
               "name":"stderr",
               "output_type":"stream",
               "text":[
                  "Using cache found in /home/yiyangl/.cache/torch/hub/facebookresearch_dinov2_main\n"
               ]
            },
            {
               "name":"stdout",
               "output_type":"stream",
               "text":[
                  "\n",
                  " \t Running on CPU.\n",
                  "\n",
                  " \t ----------- Model Loaded------------\n",
                  "\n",
                  " \t ----------- Model Loaded------------\n",
                  "\t *Total Params* =  162580481\n",
                  "\t *Trainable Params* =  9637121\n",
                  "gradient stats:\n",
                  "module.img_projection.1.weight - Mean: 0.003867906518280506, Max: 0.06775069236755371, Min:-0.06493232399225235 ,Grad Norm: 3.664635181427002\n",
                  "module.img_projection.1.bias - Mean: 0.001993519254028797, Max: 0.011902776546776295, Min:-0.009891560301184654 ,Grad Norm: 0.061202552169561386\n",
                  "module.img_projection.3.weight - Mean: 0.005116106942296028, Max: 0.07730644941329956, Min:-0.07029534876346588 ,Grad Norm: 2.7585644721984863\n",
                  "module.img_projection.3.bias - Mean: 0.0021325310226529837, Max: 0.0073379636742174625, Min:-0.006867682561278343 ,Grad Norm: 0.04250188171863556\n",
                  "module.img_projection.4.weight - Mean: 0.008235945366322994, Max: 0.04366697743535042, Min:-0.0390012301504612 ,Grad Norm: 0.17671535909175873\n",
                  "module.img_projection.4.bias - Mean: 0.005774320103228092, Max: 0.01945638097822666, Min:-0.018916262313723564 ,Grad Norm: 0.11475755274295807\n",
                  "module.txt_projection.1.weight - Mean: 0.003487542038783431, Max: 0.1511547714471817, Min:-0.19713525474071503 ,Grad Norm: 3.821377992630005\n",
                  "module.txt_projection.1.bias - Mean: 0.006631286349147558, Max: 0.04251408204436302, Min:-0.05043007805943489 ,Grad Norm: 0.2177562415599823\n",
                  "module.txt_projection.3.weight - Mean: 0.004878178704530001, Max: 0.0796845331788063, Min:-0.13977333903312683 ,Grad Norm: 3.125852108001709\n",
                  "module.txt_projection.3.bias - Mean: 0.010728511027991772, Max: 0.03272398188710213, Min:-0.047956518828868866 ,Grad Norm: 0.22683614492416382\n",
                  "module.txt_projection.4.weight - Mean: 0.008042030967772007, Max: 0.05225136876106262, Min:-0.030981890857219696 ,Grad Norm: 0.17759333550930023\n",
                  "module.txt_projection.4.bias - Mean: 0.008439315482974052, Max: 0.025956036522984505, Min:-0.036606620997190475 ,Grad Norm: 0.17780791223049164\n",
                  "module.proj_into_class.0.weight - Mean: 0.006735265254974365, Max: 0.41248226165771484, Min:-0.3665408194065094 ,Grad Norm: 34.29671096801758\n",
                  "module.proj_into_class.0.bias - Mean: 3.5719858715310693e-07, Max: 1.5497207641601562e-06, Min:-2.2649765014648438e-06 ,Grad Norm: 6.253291758184787e-06\n",
                  "module.proj_into_class.1.weight - Mean: 0.011124663054943085, Max: 0.09281057864427567, Min:-0.037261977791786194 ,Grad Norm: 0.19517114758491516\n",
                  "module.proj_into_class.1.bias - Mean: 0.03560692444443703, Max: 0.09983494877815247, Min:-0.11361975222826004 ,Grad Norm: 0.49907106161117554\n",
                  "module.proj_into_class.2.weight - Mean: 0.10076827555894852, Max: 0.25695112347602844, Min:-0.32167917490005493 ,Grad Norm: 1.3702690601348877\n",
                  "module.proj_into_class.2.bias - Mean: 0.3306444585323334, Max: 0.3306444585323334, Min:0.3306444585323334 ,Grad Norm: 0.3306444585323334\n",
                  "train_loss =  0.9846461415290833\n",
                  "\n",
                  "\t Epoch :  1\n",
                  "\n",
                  "\t Accuracy :  0.3704\n",
                  "\n",
                  "\t AUROC :  0.3486842105263158\n",
                  "\n",
                  "\t f1 :  0.35624123422159887\n",
                  "\t Training loss:  0.9528\n",
                  "\t Training time current epoch:  13.07 seconds\n",
                  "f1: 0.4166666666666667\n",
                  "AUROC: 0.3\n",
                  "prediction loss: 0.6495817518234253  accuracy:  0.7143 auroc 0.3 f1 0.4166666666666667\n"
               ]
            }
         ],
         "source":[
            "#FOR DEBUG ONLY\n",
            "clip_c.train(batch_size =12, epochs=1,load = 0, ratio = 0.005, loss = 1)"
         ]
      },
      {
         "cell_type":"code",
         "execution_count":null,
         "id":"57fa001b",
         "metadata":{
            
         },
         "outputs":[
            
         ],
         "source":[
            
         ]
      },
      {
         "cell_type":"code",
         "execution_count":null,
         "id":"1af9fdfe",
         "metadata":{
            
         },
         "outputs":[
            
         ],
         "source":[
            
         ]
      }
   ],
   "metadata":{
      "kernelspec":{
         "display_name":"conda-env",
         "language":"python",
         "name":"conda-env"
      },
      "language_info":{
         "codemirror_mode":{
            "name":"ipython",
            "version":3
         },
         "file_extension":".py",
         "mimetype":"text/x-python",
         "name":"python",
         "nbconvert_exporter":"python",
         "pygments_lexer":"ipython3",
         "version":"3.9.19"
      }
   },
   "nbformat":4,
   "nbformat_minor":5
}
